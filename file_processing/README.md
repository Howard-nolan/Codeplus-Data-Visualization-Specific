# Data Merging and Wrangling Workflow Overview

### Merging InfoUSA Data
In the infousa dataset, we are provided around 38,000 zip code files for the year 2020. Each file includes demographic data for a specific zip code in the US. Therefore, in order for us to plot visualizations for Celine’s counties of interest, Charleston and Houston County, or visualizations of the entire United States, we had to merge these zip code files together.

When we began this file merging process, we found that the number of files to be merged and the size of these files would be an issue in terms of runtime and space. Merging all ~38,000 files together as .csv files proved to be a slow process, and we were unable to load all of them at once because there were so many files. Thus, we experimented and decided to use the parquet file format; the pyarrow increased file merging times because pyarrow merging is based on relevant columns, while merging csv files looks through every row of the dataframe. By using parquet file formats and pyarrow to merge the files, merging and eventually loading the files proved to be much faster. In the end, our file merging process included merging all the zip code files in 4 parts (zip codes from 00000 to 25000, 25001 to 50000, 50001 to 75000, and 75001 to 99999). 

### Filtering through Above-Ground Storage Tanks Data
The above-ground storage tank (AST) data includes around 98,000 tanks in the US. Some of the key columns in this dataset were the longitude and latitude coordinates of the four corners of each tank. In order to plot each tank as a specific point (this will save memory), we found the averages of longitude and latitude coordinates of the four corners. We also got rid of a lot of columns that we were not going to use (this will also save space and be more memory efficient). Afterwards, in this processing file, we transformed the new average longitude and latitude coordinates from the 4326 projection to the 3857 projection so that we could plot them with gpus on future cuxfilter visualizations. Thus, the output dataframe from this master ast processing file included the longitude and latitude coordinates in both 4326 and 3857 projections, as well as tank type, state the tank is located in, and the tank geometry. 

### Number of Children per County
We have a processing file that creates a dataframe with the number of children in the households of each county. We wanted to have a dataframe containing this information so that we can make county level visualizations and display how many children there are per county on a map. In order to make this visualization, we will need geometries of the counties; we found a shapefile online that gives us county geometry. We merge this county shape file with the overall merged infousa zip code file so we can classify each zip code by county. This is important because the household infousa data is classified by zip code, which is helpful because we can use the groupby function to sum up the children's household counts in each county. The resulting data frame from this file processing includes zip code, county, state, and the total number of children in that county.

### Merging Risk and AST Data
In our visualizations, we also wanted to look at the risk factors the different natural hazards have on tanks. We were given the NRI hazards data, which included risk indexes for seven different hazards. Since these risk indexes corresponded to a certain county in the US, we needed to categorize each tank by county so that we can assign a natural hazard risk to a tank. To do so, we used spatial joins in which we took a shapefile with county geometries, and checked to see if a tank’s coordinate landed in that county’s polygon. We checked all of the tanks and all of the counties, and we were able to get an output data frame that included a column which was the county the particular tank was located in. 
Once we had the county classifications of the tanks, we merged that dataframe with the risk data by the county column. Now, we have a dataframe that includes tank coordinates, county classification, along with the hazards risk associated with each tank.

### Calculating Distances Between Households and Tanks for All Households in US
In Celine’s research, one of her focuses is how the surrounding neighborhoods near tanks would be affected due to tank spillage. Thus, we wanted to find the distances between each household and the tank nearest to it. To do so, we first must find the nearest tank to each household. Using the sklearn ball tree method, we were able to do just that. This method takes in two geo dataframes (one geodataframe includes the coordinates of all of the households, the other includes all of the tank data) and uses machine learning to find the coordinates of the tank nearest to each household in the dataframe. Having now found the lon/lat coordinates of the nearest tanks to each household, we are able to calculate the distance between the two sets of coordinates using the haversine distance method, which returns us distance in miles (it first returns as meters but we convert it to miles).

### Household and Tank Distances for Case Studies
We also wanted to visualize household and tank distance more in depth for Charleston and Harris Counties. To do so, we first filtered to only get households in such counties using the census county number (Harris county is 210 and Charleston county is 19) from the larger infousa dataset. Then, we converted this smaller dataframe into a geo dataframe so that the coordinates could be plotted using the cuxfilter library. Similarly to the paragraph above, we used the sklearn ball tree machine learning method to find the nearest tank for each household in Charleston and Harris County. This function returns the coordinates of the closest tank to each household; these coordinates will then be used to calculate the distance between them. It is important that at the end, you merge the household coordinate columns with the tank coordinate columns so that all the coordinates you want to be plotted are in one column. This is because cuxfilter dashboards, when plotting coordinates, will plot 2 columns at a time (one column for longitude, one for latitude). So, if you want households and tanks to be plotted on the same map, you should combine them into the same column, and just change the color of the points when visualizing the dashboards. 

### Household Count by County
In this file processing, we wanted to classify the households in terms of the counties they are in (infousa households come in the form of zip codes) so that we could calculate the total household counts by county. We did this using spatial joins, where we had the geometries of each county and checked to see if the coordinates of the household landed within the geometry of a certain county. We were able to loop through each of the counties and households to classify by county, and then grouped by county to get the sums of the household counts.

### Natural Hazards Breakdown and Merging
We also wanted to include natural hazard risk in our visualization of tank proximity and households. Earlier, we talked about classifying tanks by county so that we can merge the tank data with the NRI hazard data. In this output dataframe, we have now merged that dataframe with the household longitude and latitude coordinates so that we can plot tanks and households in the same dashboard. We have included the transformed (3857) and untransformed (4326) coordinates as well as the 6 different risk indexes (hurricanes, earthquakes, tornados, strong wind, coastal flooding, and riverine flooding). Moreover, since we wanted to plot each of the risks separately, we then made separate dataframes with the household and tank coordinates along with one of each of the risk columns; then, we exported these dataframes as individual parquet files. Therefore, when we want to make a visualization with only hurricane risk, we can just read in the respective parquet file and plot based on those points. 